<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BREATH: A Bio-Radar Embodied Agent for Tonal and Human-Aware Diffusion Music Generation</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>
<body>
    <!-- 导航栏 -->
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-logo">
                <i class="fas fa-music"></i>
                <span>BREATH</span>
            </div>
            <ul class="nav-menu">
                <li><a href="#home">Home</a></li>
                <li><a href="#demo">Audio Demo</a></li>
                <li><a href="#agent">System</a></li>
                <li><a href="#about">About</a></li>
            </ul>
            <div class="hamburger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
    </nav>

    <!-- 主页横幅 -->
    <section id="home" class="hero">
        <div class="hero-container">
            <div class="hero-content">
                <h1 class="paper-title">BREATH: A Bio-Radar Embodied Agent for Tonal and Human-Aware Diffusion Music Generation</h1>
                <div class="abstract-section">
                    <h3>ABSTRACT</h3>
                    <p class="abstract-text">
                        We present a multimodal system for personalized music generation that integrates physiological sensing, LLM-based reasoning, and controllable audio synthesis. A millimeter-wave radar sensor non-invasively captures heart rate and respiration rate. These physiological signals, combined with environmental state, are interpreted by a reasoning agent to infer symbolic musical descriptors, such as tempo, mood intensity, and traditional Chinese pentatonic modes, which are then expressed as structured prompts to guide a diffusion-based audio model in synthesizing expressive melodies. The system emphasizes cultural grounding through tonal embeddings and enables adaptive, embodied music interaction. To evaluate the system, we adopt a research-creation methodology combining case studies, expert feedback, and targeted control experiments. Results show that physiological variations can modulate musical features in meaningful ways, and tonal conditioning enhances alignment with intended modal characteristics. Expert users reported that the system affords intuitive, culturally resonant musical responses and highlighted its potential for therapeutic and interactive applications. This work demonstrates a novel bio-musical feedback loop linking radar-based sensing, prompt reasoning, and generative audio modeling.
                    </p>
                </div>
                <div class="hero-buttons">
                    <a href="#demo" class="btn btn-primary">Audio Demo</a>
                    <a href="#agent" class="btn btn-secondary">System</a>
                </div>
            </div>
            <div class="hero-visual">
                <div class="music-visualizer">
                    <div class="bar"></div>
                    <div class="bar"></div>
                    <div class="bar"></div>
                    <div class="bar"></div>
                    <div class="bar"></div>
                    <div class="bar"></div>
                    <div class="bar"></div>
                    <div class="bar"></div>
                </div>
            </div>
        </div>
    </section>

    <!-- 音频Demo区域 -->
    <section id="demo" class="demo-section">
        <div class="container">
            <h2 class="section-title">Audio Demo</h2>
            <p class="section-subtitle">Physiological changes modulate the mode and mood of generated music</p>
            
            <div class="audio-demos" id="audio-demos">
                <!-- 动态加载的音频demo将在这里显示 -->
            </div>

            <div class="demo-controls">
                <button class="btn btn-secondary" onclick="downloadAll()">
                    <i class="fas fa-download"></i>
                    Download All
                </button>
            </div>
        </div>
    </section>

    <!-- Agent参数表区域 -->
    <section id="agent" class="agent-section">
        <div class="container">
            <h2 class="section-title">System Architecture</h2>
            <p class="section-subtitle">Multimodal music generation workflow</p>
            
            <div class="system-content" id="system-content">
                <div class="workflow-section">
                    <h3><i class="fas fa-project-diagram"></i> System Workflow</h3>
                    <div class="workflow-image">
                        <img src="breath_flow.png" alt="BREATH System Workflow" class="workflow-diagram">
                    </div>
                </div>
                
                <div class="agent-details-section">
                    <h3><i class="fas fa-robot"></i> Agent Details</h3>
                    <div class="agent-workflow">
                        <h4>Agent决策：</h4>
                        <div class="workflow-steps">
                            <div class="workflow-step">
                                <h5><i class="fas fa-arrow-right"></i> 输入：</h5>
                                <p>生理/环境/场景、画像先验（profile）</p>
                            </div>
                            
                            <div class="workflow-step">
                                <h5><i class="fas fa-brain"></i> LLM COT:</h5>
                                <div class="cot-steps">
                                    <div class="cot-step">
                                        <strong>1. 分析状态：</strong>先解释生理信号（如心率、呼吸），再结合情境线索（如时间、温度）形成对当前用户状态的判断。
                                    </div>
                                    <div class="cot-step">
                                        <strong>2. 决定音乐意图：</strong>基于上一步判断，选择高层目标（如 transition 或 stimulation，亦可等价"calming/稳态"）。
                                    </div>
                                    <div class="cot-step">
                                        <strong>3. 生成结构化输出：</strong>把意图映射成四个可控参数——tempo / style / instrumentation / tonal mode（五声：宫商角徵羽）。
                                    </div>
                                </div>
                            </div>
                            
                            <div class="workflow-step">
                                <h5><i class="fas fa-arrow-right"></i> 输出：</h5>
                                <p>四个音乐参数——tempo(BPM) / style / instruments / mode，并给出 intent</p>
                            </div>
                        </div>
                        
                        <div class="decision-criteria">
                            <h4><i class="fas fa-clipboard-list"></i> 决策准则</h4>
                            <div class="criteria-list">
                                <div class="criteria-item">
                                    <h6><i class="fas fa-heartbeat"></i> 生理启发：</h6>
                                    <p>hr_bpm > 100 或 resp_rate > 20 → 倾向 intent="stabilize" 与镇静风格；hr_bpm < 60 → 倾向 intent="stimulate" 与提振风格；介于其间结合场景与画像。</p>
                                </div>
                                
                                <div class="criteria-item">
                                    <h6><i class="fas fa-map-marker-alt"></i> 场景启发（缺省对照）：</h6>
                                    <p>work→Shang/Gong，sleep/relax→Yu/Gong，exercise→Zhi/Jue，learn→Shang/Yu。</p>
                                </div>
                                
                                <div class="criteria-item">
                                    <h6><i class="fas fa-user-circle"></i> 画像先验：</h6>
                                    <p>按 scene + 当前时间所落 time_range 选用 tempo_bpm_range / modes_top / styles_top / instruments_default。</p>
                                </div>
                                
                                <div class="criteria-item">
                                    <h6><i class="fas fa-link"></i> 延续性：</h6>
                                    <p>若提供了 prev_mode / prev_instruments 且与当前判断不冲突，优先延续或做温和过渡。</p>
                                </div>
                                
                                <div class="criteria-item">
                                    <h6><i class="fas fa-tachometer-alt"></i> Tempo：</h6>
                                    <p>优先选画像区间内的整数 BPM（建议取区间中位数附近）；如无画像则用常识范围：sleep(60–68)、work(70–84)、relax(60–76)、exercise(90–120)。</p>
                                </div>
                                
                                <div class="criteria-item">
                                    <h6><i class="fas fa-shield-alt"></i> 输出限制：</h6>
                                    <p>BPM 在 40–180；style/mode/instruments 用受控词表；只输出 JSON。</p>
                                </div>
                            </div>
                        </div>
                        
                        <div class="fallback-section">
                            <h4><i class="fas fa-exclamation-triangle"></i> 失败与兜底</h4>
                            <div class="fallback-list">
                                <div class="fallback-item">
                                    <p>若 profile 无匹配 time_range：使用该场景的第一条先验；如仍缺失，按"场景启发 + 常识BPM"兜底。</p>
                                </div>
                                <div class="fallback-item">
                                    <p>若输入缺 prev_*：忽略延续性约束，直接按画像与场景选择。</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- 关于区域 -->
    <section id="about" class="about-section">
        <div class="container">
            <h2 class="section-title">About BREATH</h2>
            <div class="about-content">
                <div class="about-text">
                    <p>
                        BREATH is a multimodal music-generation system that turns millimetre-wave radar heart-rate & respiration signals into Chinese-pentatonic audio in real time.
                    </p>
                    <p>
                        A radar front-end captures HR/RR; an LLM agent maps these body states to musical parameters (tempo, genre, instrumentation, Gong–Yu mode); a latent-diffusion transformer synthesises 44.1 kHz stereo.
                    </p>
                    <p>
                        The clips on this page illustrate how physiological changes modulate the mode and mood of the generated music.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <!-- 页脚 -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-section">
                    <h3>BREATH</h3>
                    <p>Bio-Radar Embodied Agent for Tonal and Human-Aware Diffusion Music Generation</p>
                </div>
                <div class="footer-section">
                    <h4>Navigation</h4>
                    <ul>
                        <li><a href="#demo">Audio Demo</a></li>
                        <li><a href="#agent">System</a></li>
                        <li><a href="#about">About</a></li>
                    </ul>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2025 BREATH. Research demonstration.</p>
            </div>
        </div>
    </footer>

    <script src="audio-config.js"></script>
    <script src="script.js"></script>
</body>
</html>
