<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BREATH: A Bio-Radar Embodied Agent for Tonal and Human-Aware Diffusion Music Generation</title>
    <link rel="stylesheet" href="styles.css?v=4">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>
<body>
    <!-- 导航栏 -->
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-logo">
                <i class="fas fa-music"></i>
                <span>BREATH</span>
            </div>
            <ul class="nav-menu">
                <li><a href="#home">Home</a></li>
                <li><a href="#demo">Audio Demo</a></li>
                <li><a href="#agent">Architecture</a></li>
                <li><a href="#about">About</a></li>
            </ul>
            <div class="hamburger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
    </nav>

    <!-- 主页横幅 -->
    <section id="home" class="hero">
        <div class="hero-container">
            <div class="hero-content-full">
                <h1 class="paper-title">BREATH: A Bio-Radar Embodied Agent for Tonal and Human-Aware Diffusion Music Generation</h1>
                <div class="abstract-section">
                    <h3>ABSTRACT</h3>
                    <p class="abstract-text">
                        We present a multimodal system for personalized music generation that integrates physiological sensing, LLM-based reasoning, and controllable audio synthesis. A millimeter-wave radar sensor non-invasively captures heart rate and respiration rate. These physiological signals, combined with environmental state, are interpreted by a reasoning agent to infer symbolic musical descriptors, such as tempo, mood intensity, and traditional Chinese pentatonic modes, which are then expressed as structured prompts to guide a diffusion-based audio model in synthesizing expressive melodies. The system emphasizes cultural grounding through tonal embeddings and enables adaptive, embodied music interaction. To evaluate the system, we adopt a research-creation methodology combining case studies, expert feedback, and targeted control experiments. Results show that physiological variations can modulate musical features in meaningful ways, and tonal conditioning enhances alignment with intended modal characteristics. Expert users reported that the system affords intuitive, culturally resonant musical responses and highlighted its potential for therapeutic and interactive applications. This work demonstrates a novel bio-musical feedback loop linking radar-based sensing, prompt reasoning, and generative audio modeling.
                    </p>
                </div>
                <div class="hero-buttons">
                    <a href="#demo" class="btn btn-primary">Audio Demo</a>
                    <a href="#agent" class="btn btn-secondary">Architecture</a>
                </div>
            </div>
        </div>
    </section>

    <!-- 音频Demo区域 -->
    <section id="demo" class="demo-section">
        <div class="container">
            <h2 class="section-title">Audio Demo</h2>
            <p class="section-subtitle">Physiological changes modulate the mode and mood of generated music</p>
            
            <div class="audio-demos" id="audio-demos">
                <!-- 动态加载的音频demo将在这里显示 -->
            </div>

            <div class="demo-controls">
                <button class="btn btn-secondary" onclick="downloadAll()">
                    <i class="fas fa-download"></i>
                    Download All
                </button>
            </div>
        </div>
    </section>

    <!-- B站视频展示区域 -->
    <section id="video-demo" class="video-section">
        <div class="container">
            <h2 class="section-title">Meditation & Ambient Music</h2>
            <p class="section-subtitle">Generated audio combined with natural white noise for meditation and ambient music applications</p>
            
            <div class="video-container">
                <iframe src="//player.bilibili.com/player.html?isOutside=true&aid=114566759456760&bvid=BV1PbjMzWEWN&cid=30140075525&p=1" 
                        scrolling="no" 
                        border="0" 
                        frameborder="no" 
                        framespacing="0" 
                        allowfullscreen="true">
                </iframe>
            </div>
            
            <div class="video-description">
                <p>This video demonstrates our generated audio combined with natural white noise, designed for meditation and ambient music applications. The integration of physiological sensing with traditional Chinese pentatonic modes creates a unique therapeutic audio experience.</p>
            </div>
        </div>
    </section>

    <!-- Agent参数表区域 -->
    <section id="agent" class="agent-section">
        <div class="container">
            <h2 class="section-title">System Architecture</h2>
            <p class="section-subtitle">Multimodal music generation workflow</p>
            
            <div class="system-content" id="system-content">
                <div class="workflow-section">
                    <h3><i class="fas fa-project-diagram"></i> System Workflow</h3>
                    <div class="workflow-image">
                        <img src="breath_flow.png" alt="BREATH System Workflow" class="workflow-diagram">
                    </div>
                </div>
                
                <div class="agent-details-section">
                    <h3><i class="fas fa-robot"></i> Agent Details</h3>
                    <div class="agent-workflow">
                        <h4>Agent Decision Process:</h4>
                        <div class="workflow-steps">
                            <div class="workflow-step">
                                <h5><i class="fas fa-arrow-right"></i> Input:</h5>
                                <p>Physiological/environmental/scene data, profile priors</p>
                            </div>
                            
                            <div class="workflow-step">
                                <h5><i class="fas fa-brain"></i> LLM COT:</h5>
                                <div class="cot-steps">
                                    <div class="cot-step">
                                        <strong>1. State Analysis:</strong> Interpret physiological signals (heart rate, respiration) and combine with contextual cues (time, temperature) to form judgments about current user state.
                                    </div>
                                    <div class="cot-step">
                                        <strong>2. Musical Intent Decision:</strong> Based on previous analysis, select high-level goals (e.g., transition or stimulation, equivalent to "calming/steady state").
                                    </div>
                                    <div class="cot-step">
                                        <strong>3. Structured Output Generation:</strong> Map intent to four controllable parameters—tempo / style / instrumentation / tonal mode (pentatonic: Gong-Shang-Jue-Zhi-Yu).
                                    </div>
                                </div>
                            </div>
                            
                            <div class="workflow-step">
                                <h5><i class="fas fa-arrow-right"></i> Output:</h5>
                                <p>Four musical parameters—tempo(BPM) / style / instruments / mode, plus intent</p>
                            </div>
                        </div>
                        
                        <div class="decision-criteria">
                            <h4><i class="fas fa-clipboard-list"></i> Decision Criteria</h4>
                            <div class="criteria-list">
                                <div class="criteria-item">
                                    <h6><i class="fas fa-heartbeat"></i> Physiological Heuristics:</h6>
                                    <p>hr_bpm > 100 or resp_rate > 20 → favor intent="stabilize" with calming styles; hr_bpm < 60 → favor intent="stimulate" with uplifting styles; in between, combine with scene and profile.</p>
                                </div>
                                
                                <div class="criteria-item">
                                    <h6><i class="fas fa-map-marker-alt"></i> Scene Heuristics (Default Mapping):</h6>
                                    <p>work→Shang/Gong, sleep/relax→Yu/Gong, exercise→Zhi/Jue, learn→Shang/Yu.</p>
                                </div>
                                
                                <div class="criteria-item">
                                    <h6><i class="fas fa-user-circle"></i> Profile Priors:</h6>
                                    <p>Select tempo_bpm_range / modes_top / styles_top / instruments_default based on scene + current time within time_range.</p>
                                </div>
                                
                                <div class="criteria-item">
                                    <h6><i class="fas fa-link"></i> Continuity:</h6>
                                    <p>If prev_mode / prev_instruments are provided and don't conflict with current judgment, prioritize continuation or gentle transition.</p>
                                </div>
                                
                                <div class="criteria-item">
                                    <h6><i class="fas fa-tachometer-alt"></i> Tempo:</h6>
                                    <p>Prefer integer BPM within profile range (suggest near median); if no profile, use common sense ranges: sleep(60–68), work(70–84), relax(60–76), exercise(90–120).</p>
                                </div>
                                
                                <div class="criteria-item">
                                    <h6><i class="fas fa-shield-alt"></i> Output Constraints:</h6>
                                    <p>BPM within 40–180; style/mode/instruments use controlled vocabulary; output JSON only.</p>
                                </div>
                            </div>
                        </div>
                        
                        <div class="stability-section">
                            <h4><i class="fas fa-shield-alt"></i> System Stability Strategies</h4>
                            <div class="stability-list">
                                <div class="stability-item">
                                    <h6><i class="fas fa-clock"></i> Profile Fallback:</h6>
                                    <p>If profile has no matching time_range: use first prior for that scene; if still missing, fallback to "scene heuristics + common sense BPM".</p>
                                </div>
                                <div class="stability-item">
                                    <h6><i class="fas fa-link"></i> Continuity Fallback:</h6>
                                    <p>If input lacks prev_*: ignore continuity constraints, select directly based on profile and scene.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- 关于区域 -->
    <section id="about" class="about-section">
        <div class="container">
            <h2 class="section-title">About BREATH</h2>
            <div class="about-content">
                <div class="about-text">
                    <p>
                        BREATH is a multimodal music-generation system that turns millimetre-wave radar heart-rate & respiration signals into Chinese-pentatonic audio in real time.
                    </p>
                    <p>
                        A radar front-end captures HR/RR; an LLM agent maps these body states to musical parameters (tempo, genre, instrumentation, pentatonic mode); a latent-diffusion transformer synthesises 44.1 kHz stereo.The clips on this page illustrate how physiological changes modulate the mode and mood of the generated music.
                    </p>
                    <p>
                        This work has been accepted by LLM4Music @ ISMIR 2025.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <!-- 页脚 -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-section">
                    <h3>BREATH</h3>
                    <p>Bio-Radar Embodied Agent for Tonal and Human-Aware Diffusion Music Generation</p>
                </div>
                <div class="footer-section">
                    <h4>Navigation</h4>
                    <ul>
                        <li><a href="#demo">Audio Demo</a></li>
                        <li><a href="#agent">System</a></li>
                        <li><a href="#about">About</a></li>
                    </ul>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2025 BREATH. Research demonstration.</p>
            </div>
        </div>
    </footer>

    <script src="audio-config.js?v=2"></script>
    <script src="script.js?v=2"></script>
</body>
</html>
