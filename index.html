<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BREATH Demo - Multimodal Music Generation</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>
<body>
    <!-- å¯¼èˆªæ  -->
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-logo">
                <i class="fas fa-music"></i>
                <span>BREATH Demo</span>
            </div>
            <ul class="nav-menu">
                <li><a href="#home">Home</a></li>
                <li><a href="#demo">Audio Demo</a></li>
                <li><a href="#agent">Parameters</a></li>
                <li><a href="#about">About</a></li>
            </ul>
            <div class="hamburger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
    </nav>

    <!-- ä¸»é¡µæ¨ªå¹… -->
    <section id="home" class="hero">
        <div class="hero-container">
            <div class="hero-content">
                <h1 class="hero-title">BREATH Demo</h1>
                <p class="hero-subtitle">Multimodal Music Generation System</p>
                <div class="abstract-section">
                    <h3>ABSTRACT</h3>
                    <p class="abstract-text">
                        We present a multimodal system for personalized music generation that integrates physiological sensing, LLM-based reasoning, and controllable audio synthesis. A millimeter-wave radar sensor non-invasively captures heart rate and respiration rate. These physiological signals, combined with environmental state, are interpreted by a reasoning agent to infer symbolic musical descriptors, such as tempo, mood intensity, and traditional Chinese pentatonic modes, which are then expressed as structured prompts to guide a diffusion-based audio model in synthesizing expressive melodies. The system emphasizes cultural grounding through tonal embeddings and enables adaptive, embodied music interaction. To evaluate the system, we adopt a research-creation methodology combining case studies, expert feedback, and targeted control experiments. Results show that physiological variations can modulate musical features in meaningful ways, and tonal conditioning enhances alignment with intended modal characteristics. Expert users reported that the system affords intuitive, culturally resonant musical responses and highlighted its potential for therapeutic and interactive applications. This work demonstrates a novel bio-musical feedback loop linking radar-based sensing, prompt reasoning, and generative audio modeling.
                    </p>
                </div>
                <div class="hero-buttons">
                    <a href="#demo" class="btn btn-primary">Audio Demo</a>
                    <a href="#agent" class="btn btn-secondary">Parameters</a>
                </div>
            </div>
            <div class="hero-visual">
                <div class="music-visualizer">
                    <div class="bar"></div>
                    <div class="bar"></div>
                    <div class="bar"></div>
                    <div class="bar"></div>
                    <div class="bar"></div>
                    <div class="bar"></div>
                    <div class="bar"></div>
                    <div class="bar"></div>
                </div>
            </div>
        </div>
    </section>

    <!-- éŸ³é¢‘DemoåŒºåŸŸ -->
    <section id="demo" class="demo-section">
        <div class="container">
            <h2 class="section-title">Audio Demo</h2>
            <p class="section-subtitle">Physiological changes modulate the mode and mood of generated music</p>
            
            <div class="audio-demos" id="audio-demos">
                <!-- åŠ¨æ€åŠ è½½çš„éŸ³é¢‘demoå°†åœ¨è¿™é‡Œæ˜¾ç¤º -->
            </div>

            <div class="demo-controls">
                <button class="btn btn-secondary" onclick="downloadAll()">
                    <i class="fas fa-download"></i>
                    Download All
                </button>
            </div>
        </div>
    </section>

    <!-- Agentå‚æ•°è¡¨åŒºåŸŸ -->
    <section id="agent" class="agent-section">
        <div class="container">
            <h2 class="section-title">System Parameters</h2>
            <p class="section-subtitle">Core parameters for multimodal music generation</p>
            
            <div class="agent-parameters" id="agent-parameters">
                <div class="parameter-section">
                    <h3><i class="fas fa-database"></i> L0: Events - ä½¿ç”¨è®°å½•</h3>
                    <div class="parameter-table-container">
                        <table class="parameter-table">
                            <thead>
                                <tr>
                                    <th>å­—æ®µ</th>
                                    <th>ç±»å‹</th>
                                    <th>ç¤ºä¾‹</th>
                                    <th>è¯´æ˜ (å¯¹åº”è®ºæ–‡)</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>event_id</strong></td>
                                    <td>string</td>
                                    <td><code>evt_20250626_2230_001</code></td>
                                    <td>æ—¶é—´æ­¥ID (Timestamp ID)</td>
                                </tr>
                                <tr>
                                    <td><strong>user_id</strong></td>
                                    <td>string</td>
                                    <td><code>xiaomei</code></td>
                                    <td>ç”¨æˆ· (User)</td>
                                </tr>
                                <tr>
                                    <td><strong>time_local</strong></td>
                                    <td>datetime</td>
                                    <td><code>2025-06-26T22:30:00+08:00</code></td>
                                    <td>æ—¶åº (å¯ç”±LLM æ¨æ–­"late evening"ä½†ä¸è½åº“)</td>
                                </tr>
                                <tr>
                                    <td><strong>scene</strong></td>
                                    <td>enum</td>
                                    <td><code>sleep / work / exercise / relax</code></td>
                                    <td>åœºæ™¯æ ‡ç­¾ (Scene tag)</td>
                                </tr>
                                <tr>
                                    <td><strong>inputs.hr_bpm</strong></td>
                                    <td>int</td>
                                    <td><code>72</code></td>
                                    <td>å¿ƒç‡æ•°å€¼ (Heart rate value)</td>
                                </tr>
                                <tr>
                                    <td><strong>inputs.resp_rate</strong></td>
                                    <td>int</td>
                                    <td><code>14</code></td>
                                    <td>å‘¼å¸æ•°å€¼(æ¬¡/åˆ†) (Respiration rate value)</td>
                                </tr>
                                <tr>
                                    <td><strong>inputs.temperature_c?</strong></td>
                                    <td>int</td>
                                    <td><code>25</code></td>
                                    <td>å¯é€‰ç¯å¢ƒæ¸©åº¦(æ•°å€¼) (Optional ambient temperature)</td>
                                </tr>
                                <tr>
                                    <td><strong>inputs.user_context?</strong></td>
                                    <td>string</td>
                                    <td><code>"play calm music"</code></td>
                                    <td>å¯é€‰ç”¨æˆ·æ„å›¾ (Optional user intent)</td>
                                </tr>
                                <tr>
                                    <td><strong>inputs.prev_instruments?</strong></td>
                                    <td>string[]</td>
                                    <td><code>["pads","strings"]</code></td>
                                    <td>ä¸Šä¸€æ­¥å™¨ä¹ (Previous instruments)</td>
                                </tr>
                                <tr>
                                    <td><strong>inputs.prev_mode?</strong></td>
                                    <td>enum</td>
                                    <td><code>Gong/Shang/Jue/Zhi/Yu</code></td>
                                    <td>ä¸Šä¸€æ­¥è°ƒå¼ (Previous mode)</td>
                                </tr>
                                <tr>
                                    <td><strong>outputs.intent</strong></td>
                                    <td>enum</td>
                                    <td><code>stabilize/stimulate/transition</code></td>
                                    <td>å†³ç­–(è®ºæ–‡æ­¥éª¤2) (Decision - Paper Step 2)</td>
                                </tr>
                                <tr>
                                    <td><strong>outputs.tempo</strong></td>
                                    <td>int | enum</td>
                                    <td><code>66</code> æˆ– <code>"moderate"</code></td>
                                    <td>å››å‚ä¹‹ä¸€ (One of the four parameters)</td>
                                </tr>
                                <tr>
                                    <td><strong>outputs.style</strong></td>
                                    <td>string</td>
                                    <td><code>"ambient_relaxed_traditional_chinese"</code></td>
                                    <td>å››å‚ä¹‹ä¸€ (One of the four parameters)</td>
                                </tr>
                                <tr>
                                    <td><strong>outputs.instruments</strong></td>
                                    <td>string[]</td>
                                    <td><code>["pads", "strings"]</code></td>
                                    <td>å››å‚ä¹‹ä¸€ (One of the four parameters)</td>
                                </tr>
                                <tr>
                                    <td><strong>outputs.mode</strong></td>
                                    <td>enum</td>
                                    <td><code>Yu</code></td>
                                    <td>å››å‚ä¹‹ä¸€ (å®«å•†è§’å¾µç¾½) (One of the four parameters)</td>
                                </tr>
                                <tr>
                                    <td><strong>feedback.thumb?</strong></td>
                                    <td>enum</td>
                                    <td>ğŸ‘/ğŸ‘</td>
                                    <td>å¯é€‰åé¦ˆ (Optional feedback)</td>
                                </tr>
                                <tr>
                                    <td><strong>feedback.listen_secs?</strong></td>
                                    <td>int</td>
                                    <td><code>2100</code></td>
                                    <td>å¯é€‰è†å¬æ—¶é•¿ (Optional listening duration)</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- å…³äºåŒºåŸŸ -->
    <section id="about" class="about-section">
        <div class="container">
            <h2 class="section-title">About BREATH Demo</h2>
            <div class="about-content">
                <div class="about-text">
                    <p>
                        BREATH is a multimodal music-generation system that turns millimetre-wave radar heart-rate & respiration signals into Chinese-pentatonic audio in real time.
                    </p>
                    <p>
                        A radar front-end captures HR/RR; an LLM agent maps these body states to musical parameters (tempo, genre, instrumentation, Gongâ€“Yu mode); a latent-diffusion transformer synthesises 44.1 kHz stereo.
                    </p>
                    <p>
                        The clips on this page illustrate how physiological changes modulate the mode and mood of the generated music.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <!-- é¡µè„š -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-section">
                    <h3>BREATH Demo</h3>
                    <p>Multimodal Music Generation System</p>
                </div>
                <div class="footer-section">
                    <h4>Navigation</h4>
                    <ul>
                        <li><a href="#demo">Audio Demo</a></li>
                        <li><a href="#agent">Parameters</a></li>
                        <li><a href="#about">About</a></li>
                    </ul>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2025 BREATH Demo. Research demonstration.</p>
            </div>
        </div>
    </footer>

    <script src="audio-config.js"></script>
    <script src="script.js"></script>
</body>
</html>
