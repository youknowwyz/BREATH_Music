<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BREATH: A Bio-Radar Embodied Agent for Tonal and Human-Aware Diffusion Music Generation</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>
<body>
    <!-- 导航栏 -->
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-logo">
                <i class="fas fa-music"></i>
                <span>BREATH</span>
            </div>
            <ul class="nav-menu">
                <li><a href="#home">Home</a></li>
                <li><a href="#demo">Audio Demo</a></li>
                <li><a href="#agent">System</a></li>
                <li><a href="#about">About</a></li>
            </ul>
            <div class="hamburger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
    </nav>

    <!-- 主页横幅 -->
    <section id="home" class="hero">
        <div class="hero-container">
            <div class="hero-content">
                <h1 class="paper-title">BREATH: A Bio-Radar Embodied Agent for Tonal and Human-Aware Diffusion Music Generation</h1>
                <div class="abstract-section">
                    <h3>ABSTRACT</h3>
                    <p class="abstract-text">
                        We present a multimodal system for personalized music generation that integrates physiological sensing, LLM-based reasoning, and controllable audio synthesis. A millimeter-wave radar sensor non-invasively captures heart rate and respiration rate. These physiological signals, combined with environmental state, are interpreted by a reasoning agent to infer symbolic musical descriptors, such as tempo, mood intensity, and traditional Chinese pentatonic modes, which are then expressed as structured prompts to guide a diffusion-based audio model in synthesizing expressive melodies. The system emphasizes cultural grounding through tonal embeddings and enables adaptive, embodied music interaction. To evaluate the system, we adopt a research-creation methodology combining case studies, expert feedback, and targeted control experiments. Results show that physiological variations can modulate musical features in meaningful ways, and tonal conditioning enhances alignment with intended modal characteristics. Expert users reported that the system affords intuitive, culturally resonant musical responses and highlighted its potential for therapeutic and interactive applications. This work demonstrates a novel bio-musical feedback loop linking radar-based sensing, prompt reasoning, and generative audio modeling.
                    </p>
                </div>
                <div class="hero-buttons">
                    <a href="#demo" class="btn btn-primary">Audio Demo</a>
                    <a href="#agent" class="btn btn-secondary">System</a>
                </div>
            </div>
            <div class="hero-visual">
                <div class="music-visualizer">
                    <div class="bar"></div>
                    <div class="bar"></div>
                    <div class="bar"></div>
                    <div class="bar"></div>
                    <div class="bar"></div>
                    <div class="bar"></div>
                    <div class="bar"></div>
                    <div class="bar"></div>
                </div>
            </div>
        </div>
    </section>

    <!-- 音频Demo区域 -->
    <section id="demo" class="demo-section">
        <div class="container">
            <h2 class="section-title">Audio Demo</h2>
            <p class="section-subtitle">Physiological changes modulate the mode and mood of generated music</p>
            
            <div class="audio-demos" id="audio-demos">
                <!-- 动态加载的音频demo将在这里显示 -->
            </div>

            <div class="demo-controls">
                <button class="btn btn-secondary" onclick="downloadAll()">
                    <i class="fas fa-download"></i>
                    Download All
                </button>
            </div>
        </div>
    </section>

    <!-- Agent参数表区域 -->
    <section id="agent" class="agent-section">
        <div class="container">
            <h2 class="section-title">System Architecture</h2>
            <p class="section-subtitle">Multimodal music generation workflow</p>
            
            <div class="system-content" id="system-content">
                <div class="workflow-section">
                    <h3><i class="fas fa-project-diagram"></i> System Workflow</h3>
                    <div class="workflow-image">
                        <img src="breath_flow.png" alt="BREATH System Workflow" class="workflow-diagram">
                    </div>
                </div>
                
                <div class="agent-details-section">
                    <h3><i class="fas fa-robot"></i> Agent Details</h3>
                    <div class="agent-workflow">
                        <h4>Agent决策：</h4>
                        <div class="workflow-steps">
                            <div class="workflow-step">
                                <h5><i class="fas fa-arrow-right"></i> 输入：</h5>
                                <p>生理/环境/场景、画像先验（profile）</p>
                            </div>
                            
                            <div class="workflow-step">
                                <h5><i class="fas fa-brain"></i> LLM COT:</h5>
                                <div class="cot-steps">
                                    <div class="cot-step">
                                        <strong>1. 分析状态：</strong>先解释生理信号（如心率、呼吸），再结合情境线索（如时间、温度）形成对当前用户状态的判断。
                                    </div>
                                    <div class="cot-step">
                                        <strong>2. 决定音乐意图：</strong>基于上一步判断，选择高层目标（如 transition 或 stimulation，亦可等价"calming/稳态"）。
                                    </div>
                                    <div class="cot-step">
                                        <strong>3. 生成结构化输出：</strong>把意图映射成四个可控参数——tempo / style / instrumentation / tonal mode（五声：宫商角徵羽）。
                                    </div>
                                </div>
                            </div>
                            
                            <div class="workflow-step">
                                <h5><i class="fas fa-arrow-right"></i> 输出：</h5>
                                <p>四个音乐参数——tempo(BPM) / style / instruments / mode，并给出 intent</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- 关于区域 -->
    <section id="about" class="about-section">
        <div class="container">
            <h2 class="section-title">About BREATH</h2>
            <div class="about-content">
                <div class="about-text">
                    <p>
                        BREATH is a multimodal music-generation system that turns millimetre-wave radar heart-rate & respiration signals into Chinese-pentatonic audio in real time.
                    </p>
                    <p>
                        A radar front-end captures HR/RR; an LLM agent maps these body states to musical parameters (tempo, genre, instrumentation, Gong–Yu mode); a latent-diffusion transformer synthesises 44.1 kHz stereo.
                    </p>
                    <p>
                        The clips on this page illustrate how physiological changes modulate the mode and mood of the generated music.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <!-- 页脚 -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-section">
                    <h3>BREATH</h3>
                    <p>Bio-Radar Embodied Agent for Tonal and Human-Aware Diffusion Music Generation</p>
                </div>
                <div class="footer-section">
                    <h4>Navigation</h4>
                    <ul>
                        <li><a href="#demo">Audio Demo</a></li>
                        <li><a href="#agent">System</a></li>
                        <li><a href="#about">About</a></li>
                    </ul>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2025 BREATH. Research demonstration.</p>
            </div>
        </div>
    </footer>

    <script src="audio-config.js"></script>
    <script src="script.js"></script>
</body>
</html>
